"""
S&P 500 ML Pipeline V2 - Production DAG with All Improvements

This DAG incorporates all the latest improvements:
- Advanced feature engineering (114 features)
- Fixed merge bottleneck (100x speedup)
- Multiple model training (XGBoost, LightGBM)
- Model selection based on OOT performance
- MLflow tracking integration

Repository Location: fx-ml-pipeline/docker/airflow/dags/sp500_ml_pipeline_v2.py
"""

from datetime import datetime, timedelta
from airflow import DAG
from airflow.operators.python import PythonOperator
from airflow.operators.bash import BashOperator
from airflow.providers.docker.operators.docker import DockerOperator
from airflow.utils.task_group import TaskGroup
import pandas as pd

# Default arguments
default_args = {
    'owner': 'ml-team',
    'depends_on_past': False,
    'start_date': datetime(2025, 10, 26),
    'email_on_failure': True,
    'email_on_retry': False,
    'retries': 1,
    'retry_delay': timedelta(minutes=5),
}

# Create DAG
dag = DAG(
    'sp500_ml_pipeline_v2',
    default_args=default_args,
    description='S&P 500 ML Pipeline with All Improvements',
    schedule_interval='0 2 * * *',  # Daily at 2 AM
    catchup=False,
    tags=['ml', 'sp500', 'production'],
)

# ============================================================================
# STAGE 1: DATA COLLECTION
# ============================================================================

with TaskGroup("data_collection", tooltip="Collect market and news data", dag=dag) as data_collection:

    collect_market_data = BashOperator(
        task_id='collect_market_data',
        bash_command="""
        source /app/.venv/bin/activate && \
        python src_clean/data_collection/market_data_collector.py \
            --instrument SPX500_USD \
            --granularity M1 \
            --output data_clean/bronze/market/
        """,
        dag=dag,
    )

    collect_news_data = BashOperator(
        task_id='collect_news_data',
        bash_command="""
        source /app/.venv/bin/activate && \
        python src_clean/data_collection/news_scraper.py \
            --output data_clean/bronze/news/ \
            --max-articles 1000
        """,
        dag=dag,
    )

# ============================================================================
# STAGE 2: FEATURE ENGINEERING
# ============================================================================

with TaskGroup("feature_engineering", tooltip="Process features", dag=dag) as feature_engineering:

    # Silver layer processing
    process_technical = BashOperator(
        task_id='process_technical_features',
        bash_command="""
        source /app/.venv/bin/activate && \
        python src_clean/data_pipelines/silver/market_technical_processor.py \
            --input data_clean/bronze/market/latest.ndjson \
            --output data_clean/silver/market/technical/
        """,
        dag=dag,
    )

    process_microstructure = BashOperator(
        task_id='process_microstructure_features',
        bash_command="""
        source /app/.venv/bin/activate && \
        python src_clean/data_pipelines/silver/market_microstructure_processor.py \
            --input data_clean/bronze/market/latest.ndjson \
            --output data_clean/silver/market/microstructure/
        """,
        dag=dag,
    )

    process_volatility = BashOperator(
        task_id='process_volatility_features',
        bash_command="""
        source /app/.venv/bin/activate && \
        python src_clean/data_pipelines/silver/market_volatility_processor.py \
            --input data_clean/bronze/market/latest.ndjson \
            --output data_clean/silver/market/volatility/
        """,
        dag=dag,
    )

    # Gold layer - combine all features
    build_gold_features = BashOperator(
        task_id='build_gold_features',
        bash_command="""
        source /app/.venv/bin/activate && \
        python src_clean/data_pipelines/gold/market_gold_builder.py \
            --technical data_clean/silver/market/technical/ \
            --microstructure data_clean/silver/market/microstructure/ \
            --volatility data_clean/silver/market/volatility/ \
            --output data_clean/gold/market/features/
        """,
        dag=dag,
    )

    # Advanced feature engineering (NEW!)
    enhance_features = BashOperator(
        task_id='enhance_features',
        bash_command="""
        source /app/.venv/bin/activate && \
        python src_clean/features/advanced_feature_engineering.py \
            --input data_clean/gold/market/features/spx500_features.csv \
            --output data_clean/gold/market/features/spx500_features_enhanced.csv
        """,
        dag=dag,
    )

    [process_technical, process_microstructure, process_volatility] >> build_gold_features >> enhance_features

# ============================================================================
# STAGE 3: NEWS PROCESSING WITH FINBERT
# ============================================================================

process_news_finbert = BashOperator(
    task_id='process_news_finbert',
    bash_command="""
    source /app/.venv/bin/activate && \
    python src_clean/data_pipelines/gold/news_signal_builder.py \
        --input data_clean/bronze/news/ \
        --output data_clean/gold/news/signals/ \
        --model finbert \
        --window 60
    """,
    dag=dag,
)

# ============================================================================
# STAGE 4: LABEL GENERATION
# ============================================================================

with TaskGroup("label_generation", tooltip="Generate prediction labels", dag=dag) as label_generation:

    generate_30min_labels = BashOperator(
        task_id='generate_30min_labels',
        bash_command="""
        source /app/.venv/bin/activate && \
        python src_clean/data_pipelines/gold/label_generator.py \
            --input data_clean/gold/market/features/spx500_features_enhanced.csv \
            --output data_clean/gold/market/labels/spx500_labels_30min.csv \
            --horizon 30
        """,
        dag=dag,
    )

    generate_60min_labels = BashOperator(
        task_id='generate_60min_labels',
        bash_command="""
        source /app/.venv/bin/activate && \
        python src_clean/data_pipelines/gold/label_generator.py \
            --input data_clean/gold/market/features/spx500_features_enhanced.csv \
            --output data_clean/gold/market/labels/spx500_labels_60min.csv \
            --horizon 60
        """,
        dag=dag,
    )

# ============================================================================
# STAGE 5: MODEL TRAINING
# ============================================================================

with TaskGroup("model_training", tooltip="Train multiple models", dag=dag) as model_training:

    # Train XGBoost with original features
    train_xgboost_original = BashOperator(
        task_id='train_xgboost_original',
        bash_command="""
        source /app/.venv/bin/activate && \
        python src_clean/training/xgboost_training_pipeline_mlflow.py \
            --market-features data_clean/gold/market/features/spx500_features.csv \
            --news-signals data_clean/gold/news/signals/spx500_news_signals.csv \
            --labels data_clean/gold/market/labels/spx500_labels_30min.csv \
            --task classification \
            --output-dir data_clean/models \
            --experiment-name sp500_production
        """,
        dag=dag,
    )

    # Train XGBoost with enhanced features
    train_xgboost_enhanced = BashOperator(
        task_id='train_xgboost_enhanced',
        bash_command="""
        source /app/.venv/bin/activate && \
        python src_clean/training/xgboost_training_pipeline_mlflow.py \
            --market-features data_clean/gold/market/features/spx500_features_enhanced.csv \
            --news-signals data_clean/gold/news/signals/spx500_news_signals.csv \
            --labels data_clean/gold/market/labels/spx500_labels_30min.csv \
            --task classification \
            --output-dir data_clean/models \
            --experiment-name sp500_production_enhanced
        """,
        dag=dag,
    )

    # Train LightGBM
    train_lightgbm = BashOperator(
        task_id='train_lightgbm',
        bash_command="""
        source /app/.venv/bin/activate && \
        python src_clean/training/lightgbm_training_pipeline_mlflow.py \
            --market-features data_clean/gold/market/features/spx500_features.csv \
            --news-signals data_clean/gold/news/signals/spx500_news_signals.csv \
            --labels data_clean/gold/market/labels/spx500_labels_30min.csv \
            --task classification \
            --output-dir data_clean/models \
            --experiment-name sp500_production_lightgbm
        """,
        dag=dag,
    )

    # Train XGBoost Regression
    train_xgboost_regression = BashOperator(
        task_id='train_xgboost_regression',
        bash_command="""
        source /app/.venv/bin/activate && \
        python src_clean/training/xgboost_training_pipeline_mlflow.py \
            --market-features data_clean/gold/market/features/spx500_features.csv \
            --news-signals data_clean/gold/news/signals/spx500_news_signals.csv \
            --labels data_clean/gold/market/labels/spx500_labels_30min.csv \
            --task regression \
            --output-dir data_clean/models \
            --experiment-name sp500_production_regression
        """,
        dag=dag,
    )

# ============================================================================
# STAGE 6: MODEL SELECTION
# ============================================================================

select_best_model = BashOperator(
    task_id='select_best_model',
    bash_command="""
    source /app/.venv/bin/activate && \
    python src_clean/training/multi_experiment_selector.py \
        --experiments sp500_production sp500_production_enhanced sp500_production_lightgbm sp500_production_regression \
        --min-threshold 0.50 \
        --max-overfitting 0.25
    """,
    dag=dag,
)

# ============================================================================
# STAGE 7: MODEL DEPLOYMENT
# ============================================================================

deploy_best_model = BashOperator(
    task_id='deploy_best_model',
    bash_command="""
    source /app/.venv/bin/activate && \
    python src_clean/training/model_selector.py \
        --experiment-name sp500_production \
        --promote \
        --production-dir data_clean/models/production
    """,
    dag=dag,
)

# ============================================================================
# STAGE 8: MONITORING & ALERTING
# ============================================================================

monitor_performance = PythonOperator(
    task_id='monitor_performance',
    python_callable=lambda: print("Monitoring model performance..."),
    dag=dag,
)

# ============================================================================
# DAG DEPENDENCIES
# ============================================================================

# Data collection runs first
data_collection

# Feature engineering depends on market data
data_collection >> feature_engineering

# News processing can run in parallel with feature engineering
data_collection >> process_news_finbert

# Labels need enhanced features
feature_engineering >> label_generation

# Training needs features, labels, and news
[feature_engineering, label_generation, process_news_finbert] >> model_training

# Model selection after all training
model_training >> select_best_model

# Deploy the best model
select_best_model >> deploy_best_model

# Monitor after deployment
deploy_best_model >> monitor_performance